#!/usr/bin/env python3
"""
DataInsightæ•°æ®åº“ç®¡ç†å™¨
è´Ÿè´£æ‰€æœ‰æ•°æ®åº“è¿æ¥ã€æŸ¥è¯¢å’Œå†™å…¥æ“ä½œ
"""

import asyncio
import logging
import os
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Tuple
from decimal import Decimal
import asyncpg
from asyncpg.pool import Pool

from .exceptions import (
    DatabaseConnectionError, DatabaseQueryError, DatabaseTimeoutError,
    DataIntegrityError
)


class DatabaseManager:
    """DataInsightæ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        
        Args:
            db_config: æ•°æ®åº“é…ç½®å­—å…¸
        """
        self.config = db_config['local']
        self.logger = logging.getLogger('datainsight.database')
        self.pool: Optional[Pool] = None
        self._connection_string = self._build_connection_string()
        
        self.logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_connection_string(self) -> str:
        """æ„å»ºæ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²"""
        # æ”¯æŒç¯å¢ƒå˜é‡æ›¿æ¢
        user = self._resolve_env_var(self.config['user'])
        password = self._resolve_env_var(self.config['password'])
        
        return (
            f"postgresql://{user}:{password}@"
            f"{self.config['host']}:{self.config['port']}"
            f"/{self.config['name']}"
        )
    
    def _resolve_env_var(self, value: str) -> str:
        """è§£æç¯å¢ƒå˜é‡"""
        if isinstance(value, str) and value.startswith('${') and value.endswith('}'):
            env_var = value[2:-1]
            return os.environ.get(env_var, '')
        return value
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        try:
            # åˆ›å»ºè¿æ¥æ± ï¼ˆæ³¨æ„ï¼šasyncpgä¸æ”¯æŒretryå‚æ•°ï¼Œéœ€è¦åœ¨åº”ç”¨å±‚å¤„ç†é‡è¯•ï¼‰
            retry_attempts = self.config.get('connection_retry_attempts', 3)
            retry_delay = self.config.get('connection_retry_delay', 1)
            
            for attempt in range(retry_attempts):
                try:
                    self.pool = await asyncpg.create_pool(
                        self._connection_string,
                        min_size=self.config.get('min_pool_size', 5),
                        max_size=self.config.get('pool_size', 50),
                        command_timeout=self.config.get('command_timeout', 45),
                        max_inactive_connection_lifetime=self.config.get('max_inactive_connection_lifetime', 1800),
                        server_settings={
                            'application_name': 'datainsight'
                        }
                    )
                    break  # æˆåŠŸåˆ›å»ºè¿æ¥æ± ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    if attempt < retry_attempts - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                        self.logger.warning(f"è¿æ¥æ± åˆ›å»ºå¤±è´¥ (å°è¯• {attempt + 1}/{retry_attempts}): {e}")
                        await asyncio.sleep(retry_delay)
                    else:
                        raise  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            
            # æµ‹è¯•è¿æ¥
            async with self.pool.acquire() as conn:
                result = await conn.fetchval("SELECT 1")
                if result == 1:
                    self.logger.info("âœ… æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ")
                else:
                    raise Exception("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
                    
        except asyncio.TimeoutError as e:
            error_msg = f"æ•°æ®åº“è¿æ¥è¶…æ—¶: {e}"
            self.logger.error(f"âŒ {error_msg}")
            raise DatabaseTimeoutError(error_msg, self._connection_string)
        except asyncpg.PostgresError as e:
            error_msg = f"PostgreSQLé”™è¯¯: {e}"
            self.logger.error(f"âŒ {error_msg}")
            raise DatabaseConnectionError(error_msg, self._connection_string)
        except Exception as e:
            error_msg = f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}"
            self.logger.error(f"âŒ {error_msg}")
            raise DatabaseConnectionError(error_msg, self._connection_string)
    
    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥æ± """
        if self.pool:
            await self.pool.close()
            self.logger.info("æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
    
    async def get_coin_data_at_time(
        self, 
        target_time: datetime, 
        coin_ids: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹çš„coin_dataæ•°æ®
        
        Args:
            target_time: ç›®æ ‡æ—¶é—´
            coin_ids: æŒ‡å®šå¸ç§IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰å¸ç§
            
        Returns:
            å¸ç§æ•°æ®åˆ—è¡¨
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_conditions = ["time = $1"]
                params = [target_time]
                
                if coin_ids:
                    placeholders = ', '.join(f'${i+2}' for i in range(len(coin_ids)))
                    where_conditions.append(f"coin_id IN ({placeholders})")
                    params.extend(coin_ids)
                
                # æ ¹æ®æ—¶é—´èŒƒå›´è‡ªåŠ¨é€‰æ‹©åˆ†åŒºè¡¨æŸ¥è¯¢
                # ä½¿ç”¨çˆ¶è¡¨coin_dataï¼ŒPostgreSQLä¼šè‡ªåŠ¨è·¯ç”±åˆ°æ­£ç¡®çš„åˆ†åŒº
                query = f"""
                    SELECT coin_id, time, current_price as price, market_cap, total_volume as volume_24h
                    FROM coin_data
                    WHERE {' AND '.join(where_conditions)}
                    ORDER BY coin_id
                """
                
                rows = await conn.fetch(query, *params)
                
                result = [
                    {
                        'coin_id': row['coin_id'],
                        'time': row['time'],
                        'price': row['price'],
                        'market_cap': row['market_cap'],
                        'volume_24h': row['volume_24h']
                    }
                    for row in rows
                ]
                
                self.logger.debug(f"æŸ¥è¯¢åˆ°{len(result)}æ¡å¸ç§æ•°æ®ï¼Œæ—¶é—´: {target_time}")
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢coin_dataå¤±è´¥: {e}")
            return []
    
    async def get_indicator_data(
        self, 
        indicator_name: str, 
        target_time: datetime, 
        coin_ids: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæŒ‡æ ‡çš„æ•°æ®
        
        Args:
            indicator_name: æŒ‡æ ‡åç§°
            target_time: ç›®æ ‡æ—¶é—´
            coin_ids: æŒ‡å®šå¸ç§IDåˆ—è¡¨
            
        Returns:
            æŒ‡æ ‡æ•°æ®åˆ—è¡¨
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_conditions = ["indicator_name = $1", "time = $2"]
                params = [indicator_name, target_time]
                
                if coin_ids:
                    placeholders = ', '.join(f'${i+3}' for i in range(len(coin_ids)))
                    where_conditions.append(f"coin_id IN ({placeholders})")
                    params.extend(coin_ids)
                
                query = f"""
                    SELECT coin_id, time, indicator_name, timeframe, indicator_value
                    FROM indicator_data
                    WHERE {' AND '.join(where_conditions)}
                    ORDER BY coin_id
                """
                
                rows = await conn.fetch(query, *params)
                
                result = [
                    {
                        'coin_id': row['coin_id'],
                        'time': row['time'],
                        'indicator_name': row['indicator_name'],
                        'timeframe': row['timeframe'],
                        'indicator_value': row['indicator_value']
                    }
                    for row in rows
                ]
                
                self.logger.debug(f"æŸ¥è¯¢åˆ°{len(result)}æ¡æŒ‡æ ‡æ•°æ®: {indicator_name}ï¼Œæ—¶é—´: {target_time}")
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢indicator_dataå¤±è´¥: {e}")
            return []
    
    async def get_market_cap_data(
        self, 
        target_time: datetime, 
        coin_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        è·å–å¸‚å€¼æ•°æ®ç”¨äºåŠ æƒè®¡ç®—
        
        Args:
            target_time: ç›®æ ‡æ—¶é—´
            coin_ids: å¸ç§IDåˆ—è¡¨
            
        Returns:
            å¸‚å€¼æ•°æ®åˆ—è¡¨
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                placeholders = ', '.join(f'${i+2}' for i in range(len(coin_ids)))
                
                query = f"""
                    SELECT coin_id, market_cap
                    FROM coin_data
                    WHERE time = $1 AND coin_id IN ({placeholders})
                    ORDER BY coin_id
                """
                
                rows = await conn.fetch(query, target_time, *coin_ids)
                
                result = [
                    {
                        'coin_id': row['coin_id'],
                        'market_cap': row['market_cap']
                    }
                    for row in rows
                ]
                
                self.logger.debug(f"æŸ¥è¯¢åˆ°{len(result)}æ¡å¸‚å€¼æ•°æ®ï¼Œæ—¶é—´: {target_time}")
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¸‚å€¼æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def save_indicator_result(
        self,
        time: datetime,
        coin_id: str,
        indicator_name: str,
        timeframe: str,
        value: float
    ) -> bool:
        """
        ä¿å­˜æŒ‡æ ‡è®¡ç®—ç»“æœ
        
        Args:
            time: æ—¶é—´æˆ³
            coin_id: å¸ç§ID
            indicator_name: æŒ‡æ ‡åç§°
            timeframe: æ—¶é—´æ¡†æ¶
            value: æŒ‡æ ‡å€¼
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                # ä½¿ç”¨ ON CONFLICT å¤„ç†é‡å¤æ•°æ®
                query = """
                    INSERT INTO indicator_data (time, coin_id, indicator_name, timeframe, indicator_value)
                    VALUES ($1, $2, $3, $4, $5)
                    ON CONFLICT (time, coin_id, indicator_name, timeframe)
                    DO UPDATE SET 
                        indicator_value = EXCLUDED.indicator_value,
                        created_at = NOW()
                """
                
                await conn.execute(
                    query, 
                    time, coin_id, indicator_name, timeframe, Decimal(str(value))
                )
                
                self.logger.debug(f"ä¿å­˜æŒ‡æ ‡ç»“æœ: {indicator_name} - {coin_id} - {value:.6f}")
                return True
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜æŒ‡æ ‡ç»“æœå¤±è´¥: {e}")
            return False
    
    async def save_indicator_results_batch(
        self, 
        results: List[Dict[str, Any]]
    ) -> int:
        """
        æ‰¹é‡ä¿å­˜æŒ‡æ ‡è®¡ç®—ç»“æœ - ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨COPY FROM STDIN
        
        Args:
            results: ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«time, coin_id, indicator_name, timeframe, value
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        if not self.pool:
            await self.initialize()
        
        if not results:
            return 0
        
        try:
            async with self.pool.acquire() as conn:
                # æ–¹æ³•1: å°è¯•ä½¿ç”¨COPY FROM STDIN (æœ€å¿«çš„æ‰¹é‡æ’å…¥æ–¹å¼)
                try:
                    # å‡†å¤‡COPYæ•°æ®æ ¼å¼
                    copy_data = []
                    current_time = datetime.now(timezone.utc)
                    
                    for result in results:
                        copy_data.append([
                            result['time'],
                            result['coin_id'],
                            result['indicator_name'],
                            result['timeframe'],
                            Decimal(str(result['value'])),
                            current_time  # created_at
                        ])
                    
                    # ä½¿ç”¨COPYæ‰¹é‡æ’å…¥åˆ°ä¸´æ—¶è¡¨ï¼Œç„¶åUPSERT
                    temp_table = f"temp_indicator_batch_{int(time.time() * 1000)}"
                    
                    # åˆ›å»ºä¸´æ—¶è¡¨
                    await conn.execute(f"""
                        CREATE TEMP TABLE {temp_table} (
                            time TIMESTAMP WITH TIME ZONE,
                            coin_id TEXT,
                            indicator_name TEXT,
                            timeframe TEXT,
                            indicator_value DECIMAL(20,8),
                            created_at TIMESTAMP WITH TIME ZONE
                        )
                    """)
                    
                    # æ‰¹é‡æ’å…¥åˆ°ä¸´æ—¶è¡¨
                    await conn.copy_records_to_table(temp_table, 
                        records=copy_data,
                        columns=['time', 'coin_id', 'indicator_name', 'timeframe', 'indicator_value', 'created_at']
                    )
                    
                    # ä»ä¸´æ—¶è¡¨UPSERTåˆ°ç›®æ ‡è¡¨
                    upsert_query = f"""
                        INSERT INTO indicator_data_hot (time, coin_id, indicator_name, timeframe, indicator_value, created_at)
                        SELECT time, coin_id, indicator_name, timeframe, indicator_value, created_at
                        FROM {temp_table}
                        ON CONFLICT (time, coin_id, indicator_name, timeframe)
                        DO UPDATE SET 
                            indicator_value = EXCLUDED.indicator_value,
                            created_at = EXCLUDED.created_at
                    """
                    await conn.execute(upsert_query)
                    
                    # æ¸…ç†ä¸´æ—¶è¡¨
                    await conn.execute(f"DROP TABLE {temp_table}")
                    
                    self.logger.info(f"ğŸš€ COPYæ‰¹é‡ä¿å­˜{len(results)}æ¡æŒ‡æ ‡ç»“æœ")
                    return len(results)
                    
                except Exception as copy_error:
                    self.logger.warning(f"COPYæ–¹å¼å¤±è´¥ï¼Œå›é€€åˆ°executemany: {copy_error}")
                    
                    # æ–¹æ³•2: å›é€€åˆ°executemanyæ–¹å¼
                    query = """
                        INSERT INTO indicator_data_hot (time, coin_id, indicator_name, timeframe, indicator_value, created_at)
                        VALUES ($1, $2, $3, $4, $5, $6)
                        ON CONFLICT (time, coin_id, indicator_name, timeframe)
                        DO UPDATE SET 
                            indicator_value = EXCLUDED.indicator_value,
                            created_at = EXCLUDED.created_at
                    """
                    
                    current_time = datetime.now(timezone.utc)
                    batch_data = [
                        (
                            result['time'],
                            result['coin_id'],
                            result['indicator_name'],
                            result['timeframe'],
                            Decimal(str(result['value'])),
                            current_time
                        )
                        for result in results
                    ]
                    
                    await conn.executemany(query, batch_data)
                    
                    self.logger.info(f"âš¡ executemanyæ‰¹é‡ä¿å­˜{len(results)}æ¡æŒ‡æ ‡ç»“æœ")
                    return len(results)
                
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡ä¿å­˜æŒ‡æ ‡ç»“æœå¤±è´¥: {e}")
            return 0
    
    async def check_indicator_data_exists(
        self, 
        indicator_name: str, 
        target_time: datetime,
        coin_id: Optional[str] = None
    ) -> bool:
        """
        æ£€æŸ¥æŒ‡æ ‡æ•°æ®æ˜¯å¦å­˜åœ¨
        
        Args:
            indicator_name: æŒ‡æ ‡åç§°
            target_time: ç›®æ ‡æ—¶é—´
            coin_id: å¸ç§IDï¼ŒNoneè¡¨ç¤ºæ£€æŸ¥å…¨å±€æŒ‡æ ‡
            
        Returns:
            æ•°æ®æ˜¯å¦å­˜åœ¨
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                if coin_id:
                    query = """
                        SELECT EXISTS(
                            SELECT 1 FROM indicator_data 
                            WHERE indicator_name = $1 AND time = $2 AND coin_id = $3
                        )
                    """
                    exists = await conn.fetchval(query, indicator_name, target_time, coin_id)
                else:
                    # æ£€æŸ¥å…¨å±€æŒ‡æ ‡æˆ–ä»»æ„å¸ç§
                    query = """
                        SELECT EXISTS(
                            SELECT 1 FROM indicator_data 
                            WHERE indicator_name = $1 AND time = $2
                        )
                    """
                    exists = await conn.fetchval(query, indicator_name, target_time)
                
                return bool(exists)
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æŒ‡æ ‡æ•°æ®å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    async def get_last_indicator_time(self) -> Optional[datetime]:
        """
        è·å–æœ€åå¤„ç†çš„æŒ‡æ ‡æ—¶é—´
        
        Returns:
            æœ€åçš„æŒ‡æ ‡æ—¶é—´æˆ–None
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = "SELECT MAX(time) FROM indicator_data"
                result = await conn.fetchval(query)
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æœ€åæŒ‡æ ‡æ—¶é—´å¤±è´¥: {e}")
            return None
    
    async def check_indicator_data_exists_for_time(self, target_time: datetime) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šæ—¶é—´ç‚¹æ˜¯å¦å·²å­˜åœ¨ä»»ä½•æŒ‡æ ‡æ•°æ®
        
        Args:
            target_time: ç›®æ ‡æ—¶é—´ç‚¹
            
        Returns:
            è¯¥æ—¶é—´ç‚¹æ˜¯å¦å­˜åœ¨æŒ‡æ ‡æ•°æ®
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = """
                    SELECT EXISTS(
                        SELECT 1 FROM indicator_data 
                        WHERE time = $1
                    )
                """
                exists = await conn.fetchval(query, target_time)
                return exists
                
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ—¶é—´ç‚¹ {target_time} æŒ‡æ ‡æ•°æ®å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    async def get_latest_coin_data_time(self) -> Optional[datetime]:
        """
        è·å–æœ€æ–°çš„coin_dataæ—¶é—´
        
        Returns:
            æœ€æ–°çš„coin_dataæ—¶é—´æˆ–None
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = "SELECT MAX(time) FROM coin_data"
                result = await conn.fetchval(query)
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æœ€æ–°coin_dataæ—¶é—´å¤±è´¥: {e}")
            return None
    
    async def get_earliest_coin_data_time(self) -> Optional[datetime]:
        """
        è·å–æœ€æ—©çš„coin_dataæ—¶é—´
        
        Returns:
            æœ€æ—©çš„coin_dataæ—¶é—´æˆ–None
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = "SELECT MIN(time) FROM coin_data"
                result = await conn.fetchval(query)
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æœ€æ—©coin_dataæ—¶é—´å¤±è´¥: {e}")
            return None
    
    async def get_coin_data_time_range(self) -> Tuple[Optional[datetime], Optional[datetime]]:
        """
        è·å–coin_dataçš„æ—¶é—´èŒƒå›´
        
        Returns:
            (æœ€æ—©æ—¶é—´, æœ€æ–°æ—¶é—´) å…ƒç»„
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = "SELECT MIN(time), MAX(time) FROM coin_data"
                row = await conn.fetchrow(query)
                return row[0], row[1]
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢coin_dataæ—¶é—´èŒƒå›´å¤±è´¥: {e}")
            return None, None
    
    async def get_indicator_data_time_range(self) -> Tuple[Optional[datetime], Optional[datetime]]:
        """
        è·å–indicator_dataçš„æ—¶é—´èŒƒå›´
        
        Returns:
            (æœ€æ—©æ—¶é—´, æœ€æ–°æ—¶é—´) å…ƒç»„
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = "SELECT MIN(time), MAX(time) FROM indicator_data"
                row = await conn.fetchrow(query)
                return row[0], row[1]
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢indicator_dataæ—¶é—´èŒƒå›´å¤±è´¥: {e}")
            return None, None
    
    async def get_missing_indicator_times(
        self, 
        start_time: datetime, 
        end_time: datetime
    ) -> List[datetime]:
        """
        è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…ç¼ºå¤±çš„æŒ‡æ ‡è®¡ç®—æ—¶é—´ç‚¹
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            
        Returns:
            ç¼ºå¤±çš„æ—¶é—´ç‚¹åˆ—è¡¨
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                # ç”Ÿæˆ3åˆ†é’Ÿé—´éš”çš„æ—¶é—´åºåˆ—
                query = """
                    WITH time_series AS (
                        SELECT generate_series($1, $2, interval '3 minutes') AS time
                    ),
                    existing_times AS (
                        SELECT DISTINCT time 
                        FROM indicator_data 
                        WHERE time BETWEEN $1 AND $2
                    )
                    SELECT ts.time
                    FROM time_series ts
                    LEFT JOIN existing_times et ON ts.time = et.time
                    WHERE et.time IS NULL
                    ORDER BY ts.time
                """
                
                rows = await conn.fetch(query, start_time, end_time)
                result = [row['time'] for row in rows]
                
                self.logger.debug(f"å‘ç°{len(result)}ä¸ªç¼ºå¤±çš„æ—¶é—´ç‚¹")
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢ç¼ºå¤±æ—¶é—´ç‚¹å¤±è´¥: {e}")
            return []
    
    async def get_available_coins(self, target_time: datetime) -> List[str]:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹å¯ç”¨çš„å¸ç§åˆ—è¡¨
        
        Args:
            target_time: ç›®æ ‡æ—¶é—´
            
        Returns:
            å¸ç§IDåˆ—è¡¨
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                query = """
                    SELECT DISTINCT coin_id
                    FROM coin_data
                    WHERE time = $1
                    ORDER BY coin_id
                """
                
                rows = await conn.fetch(query, target_time)
                result = [row['coin_id'] for row in rows]
                
                self.logger.debug(f"æ—¶é—´{target_time}å¯ç”¨å¸ç§: {len(result)}ä¸ª")
                return result
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¯ç”¨å¸ç§å¤±è´¥: {e}")
            return []
    
    async def get_indicator_statistics(self) -> Dict[str, Any]:
        """
        è·å–æŒ‡æ ‡æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.pool:
            await self.initialize()
        
        try:
            async with self.pool.acquire() as conn:
                # æ€»ä½“ç»Ÿè®¡
                total_query = "SELECT COUNT(*) FROM indicator_data"
                total_count = await conn.fetchval(total_query)
                
                # æŒ‰æŒ‡æ ‡ç»Ÿè®¡
                indicator_query = """
                    SELECT indicator_name, COUNT(*) as count
                    FROM indicator_data
                    GROUP BY indicator_name
                    ORDER BY count DESC
                """
                indicator_rows = await conn.fetch(indicator_query)
                
                # æ—¶é—´èŒƒå›´ç»Ÿè®¡
                time_query = """
                    SELECT 
                        MIN(time) as earliest,
                        MAX(time) as latest,
                        COUNT(DISTINCT time) as time_points
                    FROM indicator_data
                """
                time_row = await conn.fetchrow(time_query)
                
                # å¸ç§ç»Ÿè®¡
                coin_query = """
                    SELECT COUNT(DISTINCT coin_id) as unique_coins
                    FROM indicator_data
                """
                unique_coins = await conn.fetchval(coin_query)
                
                return {
                    'total_records': total_count,
                    'unique_coins': unique_coins,
                    'time_points': time_row['time_points'],
                    'earliest_time': time_row['earliest'],
                    'latest_time': time_row['latest'],
                    'indicators': [
                        {
                            'name': row['indicator_name'],
                            'count': row['count']
                        }
                        for row in indicator_rows
                    ]
                }
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    async def health_check(self) -> bool:
        """
        æ•°æ®åº“å¥åº·æ£€æŸ¥
        
        Returns:
            å¥åº·çŠ¶æ€
        """
        if not self.pool:
            try:
                await self.initialize()
            except:
                return False
        
        try:
            async with self.pool.acquire() as conn:
                # æ£€æŸ¥æ•°æ®åº“è¿æ¥
                result = await conn.fetchval("SELECT 1")
                
                # æ£€æŸ¥æ ¸å¿ƒè¡¨æ˜¯å¦å­˜åœ¨
                table_check = await conn.fetchval("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'indicator_data'
                    )
                """)
                
                return result == 1 and table_check
                
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def get_pool_status(self) -> Dict[str, Any]:
        """
        è·å–è¿æ¥æ± çŠ¶æ€ä¿¡æ¯
        
        Returns:
            è¿æ¥æ± çŠ¶æ€å­—å…¸
        """
        if not self.pool:
            return {"status": "not_initialized"}
        
        try:
            return {
                "status": "active",
                "size": self.pool.get_size(),
                "idle_size": self.pool.get_idle_size(),
                "max_size": self.pool.get_max_size(),
                "min_size": self.pool.get_min_size()
            }
        except Exception as e:
            self.logger.error(f"è·å–è¿æ¥æ± çŠ¶æ€å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}
    
    async def cleanup_old_data(self, retention_days: int = 365) -> int:
        """
        æ¸…ç†æ—§æ•°æ® (ä»…ç”¨äºå¼€å‘æµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒåº”è¯¥ç”±æ•°æ®è¿ç§»ç³»ç»Ÿå¤„ç†)
        
        Args:
            retention_days: ä¿ç•™å¤©æ•°
            
        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        if not self.pool:
            await self.initialize()
        
        try:
            cutoff_time = datetime.now(timezone.utc) - timedelta(days=retention_days)
            
            async with self.pool.acquire() as conn:
                query = "DELETE FROM indicator_data WHERE created_at < $1"
                result = await conn.execute(query, cutoff_time)
                
                # ä»ç»“æœå­—ç¬¦ä¸²ä¸­æå–åˆ é™¤çš„è¡Œæ•°
                deleted_count = int(result.split()[-1]) if result.startswith('DELETE') else 0
                
                self.logger.info(f"æ¸…ç†äº†{deleted_count}æ¡æ—§æŒ‡æ ‡æ•°æ®")
                return deleted_count
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return 0