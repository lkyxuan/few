#!/usr/bin/env python3
"""
DataSync åŒæ­¥ç®¡ç†å™¨
è´Ÿè´£ä»è¿œç¨‹æ•°æ®åº“åŒæ­¥æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“çš„æ ¸å¿ƒé€»è¾‘
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from sqlalchemy import text, select, func
from sqlalchemy.dialects.postgresql import insert

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from database.connection import DatabaseManager
from models import CoinData, SyncLog
from logs.logger import DataSyncLogger
from monitoring.monitor_client import get_monitor_client


class SyncManager:
    """æ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, config_manager, monitor_client=None):
        """
        åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
            monitor_client: ç›‘æ§å®¢æˆ·ç«¯å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config_manager
        self.sync_config = config_manager.get_sync_config()
        self.logger = DataSyncLogger(logging.getLogger('datasync.sync'))
        
        # ç›‘æ§å®¢æˆ·ç«¯
        self.monitor = monitor_client or get_monitor_client()
        
        # ç§»é™¤è§¦å‘æ–‡ä»¶ç®¡ç†å™¨ï¼Œç°åœ¨ä½¿ç”¨PostgreSQL NOTIFYæœºåˆ¶
        
        # åŒæ­¥é…ç½®å‚æ•° - ä¼˜åŒ–ç‰ˆ
        self.batch_size = self.sync_config.get('batch_size', 10000)  # æå‡åˆ°10,000æ¡æ‰¹é‡æŸ¥è¯¢
        self.insert_batch_size = self.sync_config.get('insert_batch_size', 1000)  # ä¿æŒ1,000æ¡æ’å…¥æ‰¹æ¬¡
        self.concurrent_workers = self.sync_config.get('concurrent_workers', 10)  # å¢åŠ å¹¶å‘workeræ•°é‡
        self.retry_max = self.sync_config.get('retry_max', 3)
        
        # æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = None
        
        # åŒæ­¥çŠ¶æ€
        self.is_running = False
        self.last_sync_times = {}
    
    async def initialize(self):
        """åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨"""
        self.logger.info("åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨")
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        local_config = self.config.get_database_config('local')
        remote_config = self.config.get_database_config('remote')
        
        self.db_manager = DatabaseManager(local_config, remote_config)
        await self.db_manager.initialize()
        
        # åŠ è½½ä¸Šæ¬¡åŒæ­¥æ—¶é—´
        await self._load_last_sync_times()
        
        self.logger.info("åŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def run(self, dry_run: bool = False):
        """
        è¿è¡Œæ•°æ®åŒæ­¥
        
        Args:
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
        """
        if self.is_running:
            self.logger.warning("åŒæ­¥ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        start_time = time.time()
        
        try:
            self.logger.sync_start("æ•°æ®åŒæ­¥", dry_run=dry_run)
            
            # å‘å‡ºåŒæ­¥å¼€å§‹äº‹ä»¶
            await self.monitor.sync_started("coin_data", 0)
            
            # å‘å‡ºåŒæ­¥çŠ¶æ€ä¿¡æ¯
            await self.monitor.service_status(
                message="ğŸš€ å¼€å§‹æ•°æ®åŒæ­¥æ£€æŸ¥",
                details={
                    "action": "sync_check_start", 
                    "dry_run": dry_run
                }
            )
            
            # æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
            sync_results = await self._execute_sync_tasks(dry_run)
            
            # æ±‡æ€»åŒæ­¥ç»“æœ
            total_records = sum(result.get('records_synced', 0) for result in sync_results)
            duration = time.time() - start_time
            
            self.logger.sync_complete("æ•°æ®åŒæ­¥", duration, total_records, 
                                    tasks_completed=len(sync_results))
            
            # å‘å‡ºåŒæ­¥æˆåŠŸäº‹ä»¶ - æ˜¾ç¤ºå…·ä½“è¡¨åå’Œæ—¶é—´ä¿¡æ¯ï¼ˆä¸å†åˆ›å»ºè§¦å‘æ–‡ä»¶ï¼Œå·²åœ¨æ‰¹æ¬¡ä¸­åˆ›å»ºï¼‰
            for result in sync_results:
                if result.get('status') == 'completed' and result.get('records_synced', 0) > 0:
                    table_name = result.get('table', 'unknown_table')
                    await self.monitor.sync_success(table_name, result.get('records_synced', 0), duration)
                    self.logger.info(f"åŒæ­¥ä»»åŠ¡å®Œæˆ: {table_name} æ€»è®¡{result.get('records_synced', 0):,}æ¡è®°å½•")
            
            # è®°å½•åŒæ­¥æ—¥å¿—
            if not dry_run:
                await self._log_sync_completion(sync_results, duration)
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.sync_error("æ•°æ®åŒæ­¥", e, duration=duration)
            
            # å‘å‡ºåŒæ­¥å¤±è´¥äº‹ä»¶
            await self.monitor.sync_failure("all_tables", str(e), duration)
            
            if not dry_run:
                await self._log_sync_error("æ•°æ®åŒæ­¥", e)
            raise
        finally:
            self.is_running = False
    
    async def _execute_sync_tasks(self, dry_run: bool) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ‰€æœ‰åŒæ­¥ä»»åŠ¡
        
        Args:
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            
        Returns:
            åŒæ­¥ç»“æœåˆ—è¡¨
        """
        tasks = []
        
        # æ£€æŸ¥è¿œç¨‹æ•°æ®åº“ä¸­å­˜åœ¨çš„è¡¨
        remote_tables = await self._get_remote_tables()
        self.logger.info(f"è¿œç¨‹æ•°æ®åº“ä¸­å‘ç°çš„è¡¨: {remote_tables}")
        
        # åˆ›å»ºå¸ç§æ•°æ®åŒæ­¥ä»»åŠ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'coin_data' in remote_tables:
            tasks.append(self._sync_coin_data(dry_run))
        else:
            self.logger.warning("è¿œç¨‹æ•°æ®åº“ä¸­æ²¡æœ‰coin_dataè¡¨")
        
        
        if not tasks:
            self.logger.warning("æ²¡æœ‰å¯åŒæ­¥çš„è¡¨")
            return []
        
        # å¹¶å‘æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœå’Œå¼‚å¸¸
        sync_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"åŒæ­¥ä»»åŠ¡{i}å¤±è´¥: {result}")
                sync_results.append({
                    'task': f'task_{i}',
                    'status': 'failed',
                    'records_synced': 0,
                    'error': str(result)
                })
            else:
                sync_results.append(result)
        
        return sync_results
    
    async def _get_remote_tables(self) -> List[str]:
        """
        è·å–è¿œç¨‹æ•°æ®åº“ä¸­çš„è¡¨åˆ—è¡¨
        
        Returns:
            è¡¨ååˆ—è¡¨
        """
        try:
            query = """
            SELECT tablename 
            FROM pg_tables 
            WHERE schemaname = 'public'
            ORDER BY tablename
            """
            result = await self.db_manager.execute_remote(query, {})
            tables = [row[0] for row in result.fetchall()]
            return tables
        except Exception as e:
            self.logger.error(f"è·å–è¿œç¨‹è¡¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def _sync_coin_data(self, dry_run: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥å¸ç§æ•°æ®
        
        Args:
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸
        """
        table_name = 'coin_data'
        start_time = time.time()
        
        self.logger.info(f"å¼€å§‹åŒæ­¥{table_name}")
        
        try:
            # è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            last_sync_time = self.last_sync_times.get(table_name)
            
            # æŸ¥è¯¢éœ€è¦åŒæ­¥çš„æ•°æ®é‡
            total_records = await self._get_remote_records_count(table_name, last_sync_time)
            
            # æ— è®ºæ˜¯å¦æœ‰æ•°æ®éƒ½å‘é€çŠ¶æ€é€šçŸ¥
            await self.monitor.service_status(
                message=f"ğŸ“Š æ•°æ®åº“åŒæ­¥çŠ¶æ€æ£€æŸ¥å®Œæˆ",
                details={
                    "table": table_name,
                    "records_to_sync": total_records,
                    "last_sync_time": str(last_sync_time) if last_sync_time else "é¦–æ¬¡åŒæ­¥"
                },
                metrics={
                    "records_to_sync": total_records
                }
            )
            
            if total_records == 0:
                return {
                    'table': table_name,
                    'status': 'completed',
                    'records_synced': 0,
                    'duration': time.time() - start_time
                }
            
            self.logger.info(f"{table_name}éœ€è¦åŒæ­¥{total_records}æ¡è®°å½•")
            
            if dry_run:
                return {
                    'table': table_name,
                    'status': 'preview',
                    'records_to_sync': total_records,
                    'duration': time.time() - start_time
                }
            
            # æ‰§è¡Œåˆ†æ‰¹åŒæ­¥
            records_synced, latest_sync_time = await self._sync_table_data(table_name, last_sync_time)
            
            # æ›´æ–°åŒæ­¥æ—¶é—´
            if latest_sync_time:
                await self._update_last_sync_time(table_name, latest_sync_time)
            else:
                current_time = datetime.utcnow()
                await self._update_last_sync_time(table_name, current_time)
            
            duration = time.time() - start_time
            return {
                'table': table_name,
                'status': 'completed',
                'records_synced': records_synced,
                'duration': duration,
                'latest_sync_time': latest_sync_time
            }
            
        except Exception as e:
            self.logger.error(f"{table_name}åŒæ­¥å¤±è´¥: {e}")
            raise
    
    async def _get_remote_records_count(self, table_name: str, last_sync_time: Optional[datetime]) -> int:
        """
        è·å–è¿œç¨‹æ•°æ®åº“ä¸­éœ€è¦åŒæ­¥çš„è®°å½•æ•°
        
        Args:
            table_name: è¡¨å
            last_sync_time: ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            
        Returns:
            è®°å½•æ•°é‡
        """
        if last_sync_time:
            query = f"""
            SELECT COUNT(*) FROM {table_name} 
            WHERE time > :last_sync_time
            """
            params = {'last_sync_time': last_sync_time}
        else:
            query = f"SELECT COUNT(*) FROM {table_name}"
            params = {}
        
        result = await self.db_manager.execute_remote(query, params)
        return result.scalar()
    
    async def _sync_table_data(self, table_name: str, last_sync_time: Optional[datetime]) -> Tuple[int, Optional[datetime]]:
        """
        åŒæ­¥è¡¨æ•°æ® - ç®€åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
        
        Args:
            table_name: è¡¨å
            last_sync_time: ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            
        Returns:
            Tuple[åŒæ­¥çš„è®°å½•æ•°, æœ€æ–°åŒæ­¥æ—¶é—´]
        """
        total_synced = 0
        latest_time = None
        current_time = last_sync_time if last_sync_time else datetime(1970, 1, 1)
        
        self.logger.info(f"å¼€å§‹åŒæ­¥ {table_name}ï¼Œèµ·å§‹æ—¶é—´: {current_time}")
        
        while True:
            # æŸ¥è¯¢ä¸€æ‰¹æ•°æ®
            query = f"""
            SELECT * FROM {table_name} 
            WHERE time >= :current_time
            ORDER BY time, coin_id
            LIMIT :limit
            """
            
            result = await self.db_manager.execute_remote(query, {
                'current_time': current_time,
                'limit': self.batch_size
            })
            rows = result.fetchall()
            
            if not rows:
                break
            
            # æ’å…¥æ•°æ®
            batch_synced = await self._insert_batch_data(table_name, rows)
            total_synced += batch_synced
            
            # æ›´æ–°æ¸¸æ ‡
            last_row = rows[-1]
            latest_time = last_row[0]  # timeå­—æ®µ
            current_time = latest_time
            
            # å‘é€é€šçŸ¥
            if batch_synced > 0:
                await self._notify_datainsight_sync_done(table_name, batch_synced)
                await self.monitor.sync_progress(table_name, batch_synced, str(latest_time))
                self.logger.info(f"æ‰¹æ¬¡å®Œæˆ: {batch_synced:,}æ¡ï¼Œæœ€æ–°æ—¶é—´: {latest_time}")
            
            # å¦‚æœè¿”å›è®°å½•æ•°å°äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜å·²åŒæ­¥å®Œæ‰€æœ‰æ•°æ®
            if len(rows) < self.batch_size:
                break
        
        return total_synced, latest_time
    
    async def _insert_batch_data(self, table_name: str, rows: List[Any]) -> int:
        """
        æ‰¹é‡æ’å…¥æ•°æ®
        
        Args:
            table_name: è¡¨å
            rows: æ•°æ®è¡Œåˆ—è¡¨
            
        Returns:
            æ’å…¥çš„è®°å½•æ•°
        """
        if not rows:
            return 0
        
        # å°†è¡Œè½¬æ¢ä¸ºå­—å…¸
        data_list = []
        for row in rows:
            if hasattr(row, '_asdict'):
                data_list.append(row._asdict())
            elif hasattr(row, '_mapping'):
                data_list.append(dict(row._mapping))
            else:
                # å¦‚æœæ˜¯tupleï¼Œéœ€è¦é…åˆcolumn names
                data_list.append(dict(row))
        
        # ä½¿ç”¨UPSERTè¯­å¥å¤„ç†é‡å¤æ•°æ®
        if table_name == 'coin_data':
            insert_stmt = insert(CoinData.__table__).values(data_list)
            upsert_stmt = insert_stmt.on_conflict_do_update(
                index_elements=['time', 'coin_id'],
                set_={
                    'current_price': insert_stmt.excluded.current_price,
                    'market_cap': insert_stmt.excluded.market_cap,
                    'total_volume': insert_stmt.excluded.total_volume,
                    'price_change_percentage_24h': insert_stmt.excluded.price_change_percentage_24h,
                    'last_updated': insert_stmt.excluded.last_updated
                }
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¡¨å: {table_name}")
        
        # æ‰§è¡Œæ’å…¥
        async with self.db_manager.get_local_session() as session:
            await session.execute(upsert_stmt)
            await session.commit()
        
        return len(data_list)
    
    
    async def _load_last_sync_times(self):
        """åŠ è½½ä¸Šæ¬¡åŒæ­¥æ—¶é—´ - ç›´æ¥ä»æ•°æ®è¡¨ä¸­è·å–æœ€æ–°æ—¶é—´"""
        async with self.db_manager.get_local_session() as session:
            # æŸ¥è¯¢æ¯ä¸ªè¡¨çš„æœ€æ–°æ•°æ®æ—¶é—´ï¼ˆæ›´å¯é çš„æ–¹æ³•ï¼‰
            for table_name in ['coin_data']:
                try:
                    if table_name == 'coin_data':
                        # ç›´æ¥æŸ¥è¯¢coin_dataè¡¨çš„æœ€æ–°æ—¶é—´
                        result = await session.execute(
                            text("SELECT MAX(time) FROM coin_data")
                        )
                    
                    row = result.fetchone()
                    if row and row[0]:
                        self.last_sync_times[table_name] = row[0]
                        self.logger.info(f"{table_name}æœ¬åœ°æœ€æ–°æ—¶é—´: {row[0]}")
                    else:
                        self.logger.info(f"{table_name}è¡¨ä¸ºç©ºï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
                        
                except Exception as e:
                    self.logger.warning(f"æŸ¥è¯¢{table_name}æœ€æ–°æ—¶é—´å¤±è´¥: {e}ï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
    
    async def _update_last_sync_time(self, table_name: str, sync_time: datetime):
        """æ›´æ–°æœ€ååŒæ­¥æ—¶é—´"""
        self.last_sync_times[table_name] = sync_time
    
    async def _log_sync_completion(self, sync_results: List[Dict], duration: float):
        """è®°å½•åŒæ­¥å®Œæˆæ—¥å¿—"""
        for result in sync_results:
            if result.get('status') == 'completed':
                log_entry = SyncLog(
                    sync_type=result['table'],
                    last_sync_time=datetime.utcnow(),
                    records_synced=result['records_synced'],
                    sync_status='completed'
                )
                
                async with self.db_manager.get_local_session() as session:
                    session.add(log_entry)
                    await session.commit()
    
    async def _log_sync_error(self, sync_type: str, error: Exception):
        """è®°å½•åŒæ­¥é”™è¯¯æ—¥å¿—"""
        log_entry = SyncLog(
            sync_type=sync_type,
            last_sync_time=datetime.utcnow(),
            records_synced=0,
            sync_status='failed',
            error_message=str(error)
        )
        
        async with self.db_manager.get_local_session() as session:
            session.add(log_entry)
            await session.commit()
    
    async def get_sync_status(self) -> Dict[str, Any]:
        """
        è·å–åŒæ­¥çŠ¶æ€
        
        Returns:
            åŒæ­¥çŠ¶æ€å­—å…¸
        """
        status = {
            'is_running': self.is_running,
            'last_sync_times': self.last_sync_times.copy(),
            'config': {
                'batch_size': self.batch_size,
                'concurrent_workers': self.concurrent_workers,
                'retry_max': self.retry_max
            }
        }
        
        # è·å–æœ€è¿‘çš„åŒæ­¥æ—¥å¿—
        async with self.db_manager.get_local_session() as session:
            recent_logs = await session.execute(
                select(SyncLog)
                .order_by(SyncLog.created_at.desc())
                .limit(10)
            )
            
            status['recent_logs'] = [log.to_dict() for log in recent_logs.scalars()]
        
        return status
    
    async def _notify_datainsight_sync_done(self, table_name: str, records_synced: int):
        """
        å‘é€PostgreSQLé€šçŸ¥ç»™DataInsight
        
        Args:
            table_name: è¡¨å
            records_synced: åŒæ­¥çš„è®°å½•æ•°
        """
        try:
            async with self.db_manager.get_local_session() as session:
                # å‘é€PostgreSQLé€šçŸ¥
                await session.execute(
                    text("NOTIFY datainsight_wakeup, 'sync_done'")
                )
                await session.commit()
                
        except Exception as e:
            self.logger.error(f"å‘é€PostgreSQLé€šçŸ¥å¤±è´¥: {e}")
    
    async def close(self):
        """å…³é—­åŒæ­¥ç®¡ç†å™¨"""
        self.logger.info("å…³é—­åŒæ­¥ç®¡ç†å™¨")
        
        if self.db_manager:
            await self.db_manager.close()