#!/usr/bin/env python3
"""
DataSync åŒæ­¥ç®¡ç†å™¨
è´Ÿè´£ä»è¿œç¨‹æ•°æ®åº“åŒæ­¥æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“çš„æ ¸å¿ƒé€»è¾‘
"""

import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from sqlalchemy import text, select, func
from sqlalchemy.dialects.postgresql import insert

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from database.connection import DatabaseManager
from models import CoinData, SyncLog
from logs.logger import DataSyncLogger
from monitoring.monitor_client import get_monitor_client
# ç§»é™¤è§¦å‘æ–‡ä»¶ç®¡ç†å™¨ï¼Œç°åœ¨ä½¿ç”¨PostgreSQL NOTIFY


class SyncManager:
    """æ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, config_manager, monitor_client=None):
        """
        åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨
        
        Args:
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
            monitor_client: ç›‘æ§å®¢æˆ·ç«¯å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config_manager
        self.sync_config = config_manager.get_sync_config()
        self.logger = DataSyncLogger(logging.getLogger('datasync.sync'))
        
        # ç›‘æ§å®¢æˆ·ç«¯
        self.monitor = monitor_client or get_monitor_client()
        
        # ç§»é™¤è§¦å‘æ–‡ä»¶ç®¡ç†å™¨ï¼Œç°åœ¨ä½¿ç”¨PostgreSQL NOTIFYæœºåˆ¶
        
        # åŒæ­¥é…ç½®å‚æ•° - ä¼˜åŒ–ç‰ˆ
        self.batch_size = self.sync_config.get('batch_size', 10000)  # æå‡åˆ°10,000æ¡æ‰¹é‡æŸ¥è¯¢
        self.insert_batch_size = self.sync_config.get('insert_batch_size', 1000)  # ä¿æŒ1,000æ¡æ’å…¥æ‰¹æ¬¡
        self.concurrent_workers = self.sync_config.get('concurrent_workers', 10)  # å¢åŠ å¹¶å‘workeræ•°é‡
        self.retry_max = self.sync_config.get('retry_max', 3)
        
        # æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = None
        
        # åŒæ­¥çŠ¶æ€
        self.is_running = False
        self.last_sync_times = {}
    
    async def initialize(self):
        """åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨"""
        self.logger.info("åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨")
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        local_config = self.config.get_database_config('local')
        remote_config = self.config.get_database_config('remote')
        
        self.db_manager = DatabaseManager(local_config, remote_config)
        await self.db_manager.initialize()
        
        # åŠ è½½ä¸Šæ¬¡åŒæ­¥æ—¶é—´
        await self._load_last_sync_times()
        
        self.logger.info("åŒæ­¥ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def run(self, dry_run: bool = False):
        """
        è¿è¡Œæ•°æ®åŒæ­¥
        
        Args:
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
        """
        if self.is_running:
            self.logger.warning("åŒæ­¥ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        start_time = time.time()
        
        try:
            self.logger.sync_start("æ•°æ®åŒæ­¥", dry_run=dry_run)
            
            # å‘å‡ºåŒæ­¥å¼€å§‹äº‹ä»¶
            await self.monitor.sync_started("coin_data", 0)
            
            # å‘å‡ºåŒæ­¥çŠ¶æ€ä¿¡æ¯
            await self.monitor.service_status(
                message="ğŸš€ å¼€å§‹æ•°æ®åŒæ­¥æ£€æŸ¥",
                details={
                    "action": "sync_check_start", 
                    "dry_run": dry_run
                }
            )
            
            # æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
            sync_results = await self._execute_sync_tasks(dry_run)
            
            # æ±‡æ€»åŒæ­¥ç»“æœ
            total_records = sum(result.get('records_synced', 0) for result in sync_results)
            duration = time.time() - start_time
            
            self.logger.sync_complete("æ•°æ®åŒæ­¥", duration, total_records, 
                                    tasks_completed=len(sync_results))
            
            # å‘å‡ºåŒæ­¥æˆåŠŸäº‹ä»¶ - æ˜¾ç¤ºå…·ä½“è¡¨åå’Œæ—¶é—´ä¿¡æ¯ï¼ˆä¸å†åˆ›å»ºè§¦å‘æ–‡ä»¶ï¼Œå·²åœ¨æ‰¹æ¬¡ä¸­åˆ›å»ºï¼‰
            for result in sync_results:
                if result.get('status') == 'completed' and result.get('records_synced', 0) > 0:
                    table_name = result.get('table', 'unknown_table')
                    await self.monitor.sync_success(table_name, result.get('records_synced', 0), duration)
                    self.logger.info(f"åŒæ­¥ä»»åŠ¡å®Œæˆ: {table_name} æ€»è®¡{result.get('records_synced', 0):,}æ¡è®°å½•")
            
            # è®°å½•åŒæ­¥æ—¥å¿—
            if not dry_run:
                await self._log_sync_completion(sync_results, duration)
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.sync_error("æ•°æ®åŒæ­¥", e, duration=duration)
            
            # å‘å‡ºåŒæ­¥å¤±è´¥äº‹ä»¶
            await self.monitor.sync_failure("all_tables", str(e), duration)
            
            if not dry_run:
                await self._log_sync_error("æ•°æ®åŒæ­¥", e)
            raise
        finally:
            self.is_running = False
    
    async def _execute_sync_tasks(self, dry_run: bool) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ‰€æœ‰åŒæ­¥ä»»åŠ¡
        
        Args:
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            
        Returns:
            åŒæ­¥ç»“æœåˆ—è¡¨
        """
        tasks = []
        
        # æ£€æŸ¥è¿œç¨‹æ•°æ®åº“ä¸­å­˜åœ¨çš„è¡¨
        remote_tables = await self._get_remote_tables()
        self.logger.info(f"è¿œç¨‹æ•°æ®åº“ä¸­å‘ç°çš„è¡¨: {remote_tables}")
        
        # åˆ›å»ºå¸ç§æ•°æ®åŒæ­¥ä»»åŠ¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'coin_data' in remote_tables:
            tasks.append(self._sync_coin_data(dry_run))
        else:
            self.logger.warning("è¿œç¨‹æ•°æ®åº“ä¸­æ²¡æœ‰coin_dataè¡¨")
        
        
        if not tasks:
            self.logger.warning("æ²¡æœ‰å¯åŒæ­¥çš„è¡¨")
            return []
        
        # å¹¶å‘æ‰§è¡ŒåŒæ­¥ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœå’Œå¼‚å¸¸
        sync_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"åŒæ­¥ä»»åŠ¡{i}å¤±è´¥: {result}")
                sync_results.append({
                    'task': f'task_{i}',
                    'status': 'failed',
                    'records_synced': 0,
                    'error': str(result)
                })
            else:
                sync_results.append(result)
        
        return sync_results
    
    async def _get_remote_tables(self) -> List[str]:
        """
        è·å–è¿œç¨‹æ•°æ®åº“ä¸­çš„è¡¨åˆ—è¡¨
        
        Returns:
            è¡¨ååˆ—è¡¨
        """
        try:
            query = """
            SELECT tablename 
            FROM pg_tables 
            WHERE schemaname = 'public'
            ORDER BY tablename
            """
            result = await self.db_manager.execute_remote(query, {})
            tables = [row[0] for row in result.fetchall()]
            return tables
        except Exception as e:
            self.logger.error(f"è·å–è¿œç¨‹è¡¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    async def _sync_coin_data(self, dry_run: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥å¸ç§æ•°æ®
        
        Args:
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸
        """
        table_name = 'coin_data'
        start_time = time.time()
        
        self.logger.info(f"å¼€å§‹åŒæ­¥{table_name}")
        
        try:
            # è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            last_sync_time = self.last_sync_times.get(table_name)
            
            # æŸ¥è¯¢éœ€è¦åŒæ­¥çš„æ•°æ®é‡
            total_records = await self._get_remote_records_count(table_name, last_sync_time)
            
            # æ— è®ºæ˜¯å¦æœ‰æ•°æ®éƒ½å‘é€çŠ¶æ€é€šçŸ¥
            await self.monitor.service_status(
                message=f"ğŸ“Š æ•°æ®åº“åŒæ­¥çŠ¶æ€æ£€æŸ¥å®Œæˆ",
                details={
                    "table": table_name,
                    "records_to_sync": total_records,
                    "last_sync_time": str(last_sync_time) if last_sync_time else "é¦–æ¬¡åŒæ­¥"
                },
                metrics={
                    "records_to_sync": total_records
                }
            )
            
            if total_records == 0:
                return {
                    'table': table_name,
                    'status': 'completed',
                    'records_synced': 0,
                    'duration': time.time() - start_time
                }
            
            self.logger.info(f"{table_name}éœ€è¦åŒæ­¥{total_records}æ¡è®°å½•")
            
            if dry_run:
                return {
                    'table': table_name,
                    'status': 'preview',
                    'records_to_sync': total_records,
                    'duration': time.time() - start_time
                }
            
            # æ‰§è¡Œåˆ†æ‰¹åŒæ­¥
            records_synced, latest_sync_time = await self._sync_table_data(table_name, last_sync_time)
            
            # æ›´æ–°åŒæ­¥æ—¶é—´
            if latest_sync_time:
                await self._update_last_sync_time(table_name, latest_sync_time)
            else:
                current_time = datetime.utcnow()
                await self._update_last_sync_time(table_name, current_time)
            
            duration = time.time() - start_time
            return {
                'table': table_name,
                'status': 'completed',
                'records_synced': records_synced,
                'duration': duration,
                'latest_sync_time': latest_sync_time
            }
            
        except Exception as e:
            self.logger.error(f"{table_name}åŒæ­¥å¤±è´¥: {e}")
            raise
    
    async def _get_remote_records_count(self, table_name: str, last_sync_time: Optional[datetime]) -> int:
        """
        è·å–è¿œç¨‹æ•°æ®åº“ä¸­éœ€è¦åŒæ­¥çš„è®°å½•æ•°
        
        Args:
            table_name: è¡¨å
            last_sync_time: ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            
        Returns:
            è®°å½•æ•°é‡
        """
        if last_sync_time:
            query = f"""
            SELECT COUNT(*) FROM {table_name} 
            WHERE time > :last_sync_time
            """
            params = {'last_sync_time': last_sync_time}
        else:
            query = f"SELECT COUNT(*) FROM {table_name}"
            params = {}
        
        result = await self.db_manager.execute_remote(query, params)
        return result.scalar()
    
    async def _sync_table_data(self, table_name: str, last_sync_time: Optional[datetime]) -> Tuple[int, Optional[datetime]]:
        """
        åŒæ­¥è¡¨æ•°æ® - ä¿®å¤ç‰ˆæœ¬ï¼Œç¡®ä¿ä¸é—æ¼æ•°æ®
        
        Args:
            table_name: è¡¨å
            last_sync_time: ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            
        Returns:
            Tuple[åŒæ­¥çš„è®°å½•æ•°, æœ€æ–°åŒæ­¥æ—¶é—´]
        """
        total_synced = 0
        latest_time = None
        
        # åŸºäºå¤åˆä¸»é”®çš„åˆ†é¡µåŒæ­¥ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
        current_time = last_sync_time if last_sync_time else datetime(1970, 1, 1)
        current_coin_id = ""  # ç”¨äºå¤„ç†ç›¸åŒæ—¶é—´æˆ³çš„è®°å½•
        
        self.logger.info(f"åŒæ­¥èµ·å§‹æ—¶é—´: {current_time}, èµ·å§‹coin_id: {current_coin_id}")
        
        batch_round = 0
        while True:
            batch_round += 1
            self.logger.info(f"å¼€å§‹ç¬¬{batch_round}è½®æ‰¹é‡æŸ¥è¯¢ï¼ˆæ‰¹æ¬¡å¤§å°: {self.batch_size:,}æ¡ï¼‰")
            # ä½¿ç”¨å¤åˆä¸»é”®åˆ†é¡µï¼Œç¡®ä¿ä¸é—æ¼åŒä¸€æ—¶é—´æˆ³çš„æ•°æ®
            if current_coin_id:
                # å¤„ç†ç›¸åŒæ—¶é—´æˆ³çš„åç»­è®°å½•
                query = f"""
                SELECT * FROM {table_name} 
                WHERE (time = :current_time AND coin_id > :current_coin_id)
                   OR time > :current_time
                ORDER BY time, coin_id
                LIMIT :limit
                """
                params = {
                    'current_time': current_time,
                    'current_coin_id': current_coin_id,
                    'limit': self.batch_size
                }
            else:
                # åˆå§‹æŸ¥è¯¢æˆ–æ–°æ—¶é—´æˆ³
                query = f"""
                SELECT * FROM {table_name} 
                WHERE time >= :current_time
                ORDER BY time, coin_id
                LIMIT :limit
                """
                params = {
                    'current_time': current_time,
                    'limit': self.batch_size
                }
            
            # ä»è¿œç¨‹æ•°æ®åº“æŸ¥è¯¢
            result = await self.db_manager.execute_remote(query, params)
            rows = result.fetchall()
            
            if not rows:
                break
            
            # åˆ†æ‰¹å¹¶å‘æ’å…¥åˆ°æœ¬åœ°æ•°æ®åº“ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
            batch_synced = await self._insert_batch_data_concurrent(table_name, rows)
            total_synced += batch_synced
            
            # æ›´æ–°æ¸¸æ ‡åˆ°æœ€åä¸€è¡Œ
            last_row = rows[-1]
            last_row_time = last_row[0]  # timeå­—æ®µ
            last_row_coin_id = last_row[2]  # coin_idå­—æ®µ
            
            latest_time = last_row_time  # è®°å½•æœ€æ–°åŒæ­¥æ—¶é—´
            
            # æ¯å®Œæˆä¸€ä¸ªæ‰¹æ¬¡åŒæ­¥åç«‹å³å‘é€é€šçŸ¥
            if batch_synced > 0:
                # 1. å‘é€æ•°æ®åº“é€šçŸ¥ç»™DataInsight
                await self._notify_datainsight_sync_done(table_name, batch_synced)
                self.logger.info(f"âœ… å·²å‘é€PostgreSQLé€šçŸ¥ç»™DataInsight: {table_name} åŒæ­¥{batch_synced:,}æ¡")
                
                # 2. å‘é€Monitoré€šçŸ¥ï¼ˆæ‰¹æ¬¡çº§åˆ«ï¼‰
                batch_duration = time.time() - time.time()  # å¯ä»¥æ›´ç²¾ç¡®è®¡ç®—æ‰¹æ¬¡æ—¶é—´
                # ä½¿ç”¨å®é™…çš„æœ€æ–°åŒæ­¥æ—¶é—´
                await self.monitor.sync_progress(table_name, batch_synced, str(latest_time))
                self.logger.info(f"ğŸ“Š å·²å‘é€Monitorè¿›åº¦é€šçŸ¥: {table_name} åŒæ­¥åˆ° {latest_time}")
                
                # ä¹Ÿæ›´æ–°æœ¬åœ°æ—¥å¿—ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ—¶é—´è€Œä¸åªæ˜¯è®°å½•æ•°
                self.logger.info(f"ğŸ“Š æ‰¹æ¬¡å®Œæˆ: {batch_synced:,}æ¡ï¼Œæœ€æ–°æ—¶é—´: {latest_time}")
            
            # æ›´æ–°åˆ†é¡µæ¸¸æ ‡
            if last_row_time == current_time:
                # ç›¸åŒæ—¶é—´æˆ³ï¼Œæ›´æ–°coin_idæ¸¸æ ‡
                current_coin_id = last_row_coin_id
            else:
                # æ–°æ—¶é—´æˆ³ï¼Œé‡ç½®coin_idæ¸¸æ ‡
                current_time = last_row_time
                current_coin_id = ""
            
            # ç§»é™¤åŸæ¥çš„3åˆ†é’Ÿè¿›åº¦é€šçŸ¥ï¼Œç°åœ¨æ¯æ‰¹æ¬¡éƒ½å‘é€é€šçŸ¥
            
            # è¿ç»­è¿½èµ¶é€»è¾‘ï¼šåªæœ‰å½“è¿”å›è®°å½•æ•°å°äºæ‰¹æ¬¡å¤§å°æ—¶æ‰åœæ­¢
            # è¿™æ ·å¯ä»¥åœ¨æ•°æ®ç§¯å‹æ—¶è¿ç»­åŒæ­¥ï¼Œåœ¨è¿½ä¸Šè¿›åº¦æ—¶è‡ªåŠ¨åœæ­¢
            if len(rows) < self.batch_size:
                self.logger.info(f"æœ¬æ‰¹æ¬¡ä»…åŒæ­¥{len(rows)}æ¡è®°å½• < {self.batch_size}æ¡ï¼Œå·²è¿½ä¸Šè¿›åº¦ï¼Œåœæ­¢åŒæ­¥")
                break
            else:
                self.logger.info(f"æœ¬æ‰¹æ¬¡åŒæ­¥æ»¡{len(rows)}æ¡è®°å½•ï¼Œå¯èƒ½è¿˜æœ‰æ›´å¤šæ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€è½®æŸ¥è¯¢")
        
        return total_synced, latest_time
    
    async def _insert_batch_data(self, table_name: str, rows: List[Any]) -> int:
        """
        æ‰¹é‡æ’å…¥æ•°æ®
        
        Args:
            table_name: è¡¨å
            rows: æ•°æ®è¡Œåˆ—è¡¨
            
        Returns:
            æ’å…¥çš„è®°å½•æ•°
        """
        if not rows:
            return 0
        
        # å°†è¡Œè½¬æ¢ä¸ºå­—å…¸
        data_list = []
        for row in rows:
            if hasattr(row, '_asdict'):
                data_list.append(row._asdict())
            elif hasattr(row, '_mapping'):
                data_list.append(dict(row._mapping))
            else:
                # å¦‚æœæ˜¯tupleï¼Œéœ€è¦é…åˆcolumn names
                data_list.append(dict(row))
        
        # ä½¿ç”¨UPSERTè¯­å¥å¤„ç†é‡å¤æ•°æ®
        if table_name == 'coin_data':
            insert_stmt = insert(CoinData.__table__).values(data_list)
            upsert_stmt = insert_stmt.on_conflict_do_update(
                index_elements=['time', 'coin_id'],
                set_={
                    'current_price': insert_stmt.excluded.current_price,
                    'market_cap': insert_stmt.excluded.market_cap,
                    'total_volume': insert_stmt.excluded.total_volume,
                    'price_change_percentage_24h': insert_stmt.excluded.price_change_percentage_24h,
                    'last_updated': insert_stmt.excluded.last_updated
                }
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¡¨å: {table_name}")
        
        # æ‰§è¡Œæ’å…¥
        async with self.db_manager.get_local_session() as session:
            await session.execute(upsert_stmt)
            await session.commit()
        
        return len(data_list)
    
    async def _insert_batch_data_concurrent(self, table_name: str, rows: List[Any]) -> int:
        """
        å¹¶å‘æ‰¹é‡æ’å…¥æ•°æ® - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
        å°†å¤§æ‰¹é‡æ•°æ®(10,000æ¡)åˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡(1,000æ¡)å¹¶å‘æ’å…¥
        
        Args:
            table_name: è¡¨å
            rows: æ•°æ®è¡Œåˆ—è¡¨ï¼ˆé€šå¸¸10,000æ¡ï¼‰
            
        Returns:
            æ’å…¥çš„è®°å½•æ•°
        """
        if not rows:
            return 0
            
        total_rows = len(rows)
        
        # å°†å¤§æ‰¹é‡æ•°æ®åˆ†æˆå°æ‰¹æ¬¡
        batches = []
        for i in range(0, total_rows, self.insert_batch_size):
            batch = rows[i:i + self.insert_batch_size]
            batches.append(batch)
        
        self.logger.info(f"å¼€å§‹å¹¶å‘æ’å…¥{total_rows}æ¡æ•°æ®ï¼Œåˆ†æˆ{len(batches)}ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡{self.insert_batch_size}æ¡")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡çš„æ’å…¥ä»»åŠ¡
        insert_tasks = []
        for i, batch in enumerate(batches):
            task = self._insert_single_batch(table_name, batch, i+1)
            insert_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æ’å…¥ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*insert_tasks, return_exceptions=True)
        
        # ç»Ÿè®¡æˆåŠŸæ’å…¥çš„è®°å½•æ•°
        total_inserted = 0
        failed_batches = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"æ‰¹æ¬¡{i+1}æ’å…¥å¤±è´¥: {result}")
                failed_batches += 1
            else:
                total_inserted += result
        
        if failed_batches > 0:
            self.logger.warning(f"å¹¶å‘æ’å…¥å®Œæˆï¼ŒæˆåŠŸæ’å…¥{total_inserted}æ¡ï¼Œ{failed_batches}ä¸ªæ‰¹æ¬¡å¤±è´¥")
        else:
            self.logger.info(f"å¹¶å‘æ’å…¥å®Œæˆï¼ŒæˆåŠŸæ’å…¥{total_inserted}æ¡ï¼Œæ‰€æœ‰æ‰¹æ¬¡æˆåŠŸ")
        
        return total_inserted
    
    async def _insert_single_batch(self, table_name: str, rows: List[Any], batch_number: int) -> int:
        """
        æ’å…¥å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        
        Args:
            table_name: è¡¨å
            rows: æ•°æ®è¡Œåˆ—è¡¨
            batch_number: æ‰¹æ¬¡ç¼–å·ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            æ’å…¥çš„è®°å½•æ•°
        """
        if not rows:
            return 0
        
        try:
            # å°†è¡Œè½¬æ¢ä¸ºå­—å…¸
            data_list = []
            for row in rows:
                if hasattr(row, '_asdict'):
                    data_list.append(row._asdict())
                elif hasattr(row, '_mapping'):
                    data_list.append(dict(row._mapping))
                else:
                    # å¦‚æœæ˜¯tupleï¼Œéœ€è¦é…åˆcolumn names
                    data_list.append(dict(row))
            
            # ä½¿ç”¨UPSERTè¯­å¥å¤„ç†é‡å¤æ•°æ®
            if table_name == 'coin_data':
                insert_stmt = insert(CoinData.__table__).values(data_list)
                upsert_stmt = insert_stmt.on_conflict_do_update(
                    index_elements=['time', 'coin_id'],
                    set_={
                        'current_price': insert_stmt.excluded.current_price,
                        'market_cap': insert_stmt.excluded.market_cap,
                        'total_volume': insert_stmt.excluded.total_volume,
                        'price_change_percentage_24h': insert_stmt.excluded.price_change_percentage_24h,
                        'last_updated': insert_stmt.excluded.last_updated
                    }
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¡¨å: {table_name}")
            
            # æ‰§è¡Œæ’å…¥
            async with self.db_manager.get_local_session() as session:
                await session.execute(upsert_stmt)
                await session.commit()
            
            self.logger.debug(f"æ‰¹æ¬¡{batch_number}æ’å…¥æˆåŠŸ: {len(data_list)}æ¡è®°å½•")
            return len(data_list)
            
        except Exception as e:
            self.logger.error(f"æ‰¹æ¬¡{batch_number}æ’å…¥å¤±è´¥: {e}")
            raise
    
    async def _load_last_sync_times(self):
        """åŠ è½½ä¸Šæ¬¡åŒæ­¥æ—¶é—´ - ç›´æ¥ä»æ•°æ®è¡¨ä¸­è·å–æœ€æ–°æ—¶é—´"""
        async with self.db_manager.get_local_session() as session:
            # æŸ¥è¯¢æ¯ä¸ªè¡¨çš„æœ€æ–°æ•°æ®æ—¶é—´ï¼ˆæ›´å¯é çš„æ–¹æ³•ï¼‰
            for table_name in ['coin_data']:
                try:
                    if table_name == 'coin_data':
                        # ç›´æ¥æŸ¥è¯¢coin_dataè¡¨çš„æœ€æ–°æ—¶é—´
                        result = await session.execute(
                            text("SELECT MAX(time) FROM coin_data")
                        )
                    
                    row = result.fetchone()
                    if row and row[0]:
                        self.last_sync_times[table_name] = row[0]
                        self.logger.info(f"{table_name}æœ¬åœ°æœ€æ–°æ—¶é—´: {row[0]}")
                    else:
                        self.logger.info(f"{table_name}è¡¨ä¸ºç©ºï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
                        
                except Exception as e:
                    self.logger.warning(f"æŸ¥è¯¢{table_name}æœ€æ–°æ—¶é—´å¤±è´¥: {e}ï¼Œå°†è¿›è¡Œå…¨é‡åŒæ­¥")
    
    async def _update_last_sync_time(self, table_name: str, sync_time: datetime):
        """æ›´æ–°æœ€ååŒæ­¥æ—¶é—´"""
        self.last_sync_times[table_name] = sync_time
    
    async def _log_sync_completion(self, sync_results: List[Dict], duration: float):
        """è®°å½•åŒæ­¥å®Œæˆæ—¥å¿—"""
        for result in sync_results:
            if result.get('status') == 'completed':
                log_entry = SyncLog(
                    sync_type=result['table'],
                    last_sync_time=datetime.utcnow(),
                    records_synced=result['records_synced'],
                    sync_status='completed'
                )
                
                async with self.db_manager.get_local_session() as session:
                    session.add(log_entry)
                    await session.commit()
    
    async def _log_sync_error(self, sync_type: str, error: Exception):
        """è®°å½•åŒæ­¥é”™è¯¯æ—¥å¿—"""
        log_entry = SyncLog(
            sync_type=sync_type,
            last_sync_time=datetime.utcnow(),
            records_synced=0,
            sync_status='failed',
            error_message=str(error)
        )
        
        async with self.db_manager.get_local_session() as session:
            session.add(log_entry)
            await session.commit()
    
    async def get_sync_status(self) -> Dict[str, Any]:
        """
        è·å–åŒæ­¥çŠ¶æ€
        
        Returns:
            åŒæ­¥çŠ¶æ€å­—å…¸
        """
        status = {
            'is_running': self.is_running,
            'last_sync_times': self.last_sync_times.copy(),
            'config': {
                'batch_size': self.batch_size,
                'concurrent_workers': self.concurrent_workers,
                'retry_max': self.retry_max
            }
        }
        
        # è·å–æœ€è¿‘çš„åŒæ­¥æ—¥å¿—
        async with self.db_manager.get_local_session() as session:
            recent_logs = await session.execute(
                select(SyncLog)
                .order_by(SyncLog.created_at.desc())
                .limit(10)
            )
            
            status['recent_logs'] = [log.to_dict() for log in recent_logs.scalars()]
        
        return status
    
    async def _notify_datainsight_sync_done(self, table_name: str, records_synced: int):
        """
        å‘é€PostgreSQLé€šçŸ¥ç»™DataInsight
        
        Args:
            table_name: è¡¨å
            records_synced: åŒæ­¥çš„è®°å½•æ•°
        """
        try:
            async with self.db_manager.get_local_session() as session:
                # å‘é€PostgreSQLé€šçŸ¥
                await session.execute(
                    text("NOTIFY datainsight_wakeup, 'sync_done'")
                )
                await session.commit()
                
        except Exception as e:
            self.logger.error(f"å‘é€PostgreSQLé€šçŸ¥å¤±è´¥: {e}")
    
    async def close(self):
        """å…³é—­åŒæ­¥ç®¡ç†å™¨"""
        self.logger.info("å…³é—­åŒæ­¥ç®¡ç†å™¨")
        
        if self.db_manager:
            await self.db_manager.close()